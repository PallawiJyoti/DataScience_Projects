{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAND WRITTEN DIGIT RECOGNITION-Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to develop a model that can correctly identify the digit (between 0-9) written in an image.\n",
    "\n",
    "Objective : We are required to develop a model using Support Vector Machine which should correctly classify the handwritten digits from 0-9 based on the pixel values given as features. \n",
    "Thus, this is a 10-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit= pd.read_csv(r'C:\\Users\\engel\\Desktop\\UpGrad\\Predictive_analysis_II/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (42000, 785) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About the dataset\n",
    "print('Dimensions:', digit.shape,'\\n')\n",
    "print(digit.info())\n",
    "digit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking unique labels\n",
    "order = list(np.sort(digit['label'].unique()))\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for missing values in the entire dataset\n",
    "digit.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.00000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.456643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219286</td>\n",
       "      <td>0.117095</td>\n",
       "      <td>0.059024</td>\n",
       "      <td>0.02019</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.887730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.312890</td>\n",
       "      <td>4.633819</td>\n",
       "      <td>3.274488</td>\n",
       "      <td>1.75987</td>\n",
       "      <td>1.894498</td>\n",
       "      <td>0.414264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.00000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
       "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
       "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
       "count  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n",
       "mean       0.0      0.0      0.0  ...      0.219286      0.117095   \n",
       "std        0.0      0.0      0.0  ...      6.312890      4.633819   \n",
       "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "\n",
       "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
       "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
       "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
       "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
       "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "25%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "75%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
       "\n",
       "       pixel781  pixel782  pixel783  \n",
       "count   42000.0   42000.0   42000.0  \n",
       "mean        0.0       0.0       0.0  \n",
       "std         0.0       0.0       0.0  \n",
       "min         0.0       0.0       0.0  \n",
       "25%         0.0       0.0       0.0  \n",
       "50%         0.0       0.0       0.0  \n",
       "75%         0.0       0.0       0.0  \n",
       "max         0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the basic statistics of all the columns\n",
    "digit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.00000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.456643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219286</td>\n",
       "      <td>0.117095</td>\n",
       "      <td>0.059024</td>\n",
       "      <td>0.02019</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.887730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.312890</td>\n",
       "      <td>4.633819</td>\n",
       "      <td>3.274488</td>\n",
       "      <td>1.75987</td>\n",
       "      <td>1.894498</td>\n",
       "      <td>0.414264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.00000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.00000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
       "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
       "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "10%        1.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "20%        1.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "30%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "40%        3.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "60%        5.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "70%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "80%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "90%        8.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "95%        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "96%        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "97%        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "98%        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "99%        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "100%       9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
       "count  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n",
       "mean       0.0      0.0      0.0  ...      0.219286      0.117095   \n",
       "std        0.0      0.0      0.0  ...      6.312890      4.633819   \n",
       "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "10%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "20%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "30%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "40%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "60%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "70%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "80%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "90%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "95%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "96%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "97%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "98%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "99%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "100%       0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "\n",
       "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
       "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
       "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
       "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
       "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "10%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "20%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "30%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "40%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "60%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "70%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "80%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "90%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "95%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "96%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "97%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "98%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "99%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "100%     253.000000    253.00000    254.000000     62.000000       0.0   \n",
       "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
       "\n",
       "       pixel781  pixel782  pixel783  \n",
       "count   42000.0   42000.0   42000.0  \n",
       "mean        0.0       0.0       0.0  \n",
       "std         0.0       0.0       0.0  \n",
       "min         0.0       0.0       0.0  \n",
       "10%         0.0       0.0       0.0  \n",
       "20%         0.0       0.0       0.0  \n",
       "30%         0.0       0.0       0.0  \n",
       "40%         0.0       0.0       0.0  \n",
       "50%         0.0       0.0       0.0  \n",
       "60%         0.0       0.0       0.0  \n",
       "70%         0.0       0.0       0.0  \n",
       "80%         0.0       0.0       0.0  \n",
       "90%         0.0       0.0       0.0  \n",
       "95%         0.0       0.0       0.0  \n",
       "96%         0.0       0.0       0.0  \n",
       "97%         0.0       0.0       0.0  \n",
       "98%         0.0       0.0       0.0  \n",
       "99%         0.0       0.0       0.0  \n",
       "100%        0.0       0.0       0.0  \n",
       "max         0.0       0.0       0.0  \n",
       "\n",
       "[20 rows x 785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for presence of any outliers\n",
    "digit_desc=digit.describe(percentiles=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.96,0.97,0.98,0.99,1])\n",
    "digit_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the above ouptut there no outliers, hence no need of treating the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function countplot in module seaborn.categorical:\n",
      "\n",
      "countplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, dodge=True, ax=None, **kwargs)\n",
      "    Show the counts of observations in each categorical bin using bars.\n",
      "    \n",
      "    A count plot can be thought of as a histogram across a categorical, instead\n",
      "    of quantitative, variable. The basic API and options are identical to those\n",
      "    for :func:`barplot`, so you can compare counts across nested variables.\n",
      "    \n",
      "    \n",
      "    Input data can be passed in a variety of formats, including:\n",
      "    \n",
      "    - Vectors of data represented as lists, numpy arrays, or pandas Series\n",
      "      objects passed directly to the ``x``, ``y``, and/or ``hue`` parameters.\n",
      "    - A \"long-form\" DataFrame, in which case the ``x``, ``y``, and ``hue``\n",
      "      variables will determine how the data are plotted.\n",
      "    - A \"wide-form\" DataFrame, such that each numeric column will be plotted.\n",
      "    - An array or list of vectors.\n",
      "    \n",
      "    In most cases, it is possible to use numpy or Python objects, but pandas\n",
      "    objects are preferable because the associated names will be used to\n",
      "    annotate the axes. Additionally, you can use Categorical types for the\n",
      "    grouping variables to control the order of plot elements.    \n",
      "    \n",
      "    This function always treats one of the variables as categorical and\n",
      "    draws data at ordinal positions (0, 1, ... n) on the relevant axis, even\n",
      "    when the data has a numeric or date type.\n",
      "    \n",
      "    See the :ref:`tutorial <categorical_tutorial>` for more information.    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x, y, hue : names of variables in ``data`` or vector data, optional\n",
      "        Inputs for plotting long-form data. See examples for interpretation.        \n",
      "    data : DataFrame, array, or list of arrays, optional\n",
      "        Dataset for plotting. If ``x`` and ``y`` are absent, this is\n",
      "        interpreted as wide-form. Otherwise it is expected to be long-form.    \n",
      "    order, hue_order : lists of strings, optional\n",
      "        Order to plot the categorical levels in, otherwise the levels are\n",
      "        inferred from the data objects.        \n",
      "    orient : \"v\" | \"h\", optional\n",
      "        Orientation of the plot (vertical or horizontal). This is usually\n",
      "        inferred from the dtype of the input variables, but can be used to\n",
      "        specify when the \"categorical\" variable is a numeric or when plotting\n",
      "        wide-form data.    \n",
      "    color : matplotlib color, optional\n",
      "        Color for all of the elements, or seed for a gradient palette.    \n",
      "    palette : palette name, list, or dict, optional\n",
      "        Colors to use for the different levels of the ``hue`` variable. Should\n",
      "        be something that can be interpreted by :func:`color_palette`, or a\n",
      "        dictionary mapping hue levels to matplotlib colors.    \n",
      "    saturation : float, optional\n",
      "        Proportion of the original saturation to draw colors at. Large patches\n",
      "        often look better with slightly desaturated colors, but set this to\n",
      "        ``1`` if you want the plot colors to perfectly match the input color\n",
      "        spec.    \n",
      "    dodge : bool, optional\n",
      "        When hue nesting is used, whether elements should be shifted along the\n",
      "        categorical axis.    \n",
      "    ax : matplotlib Axes, optional\n",
      "        Axes object to draw the plot onto, otherwise uses the current Axes.    \n",
      "    kwargs : key, value mappings\n",
      "        Other keyword arguments are passed to ``plt.bar``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ax : matplotlib Axes\n",
      "        Returns the Axes object with the plot drawn onto it.    \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    barplot : Show point estimates and confidence intervals using bars.    \n",
      "    catplot : Combine a categorical plot with a class:`FacetGrid`.    \n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    Show value counts for a single categorical variable:\n",
      "    \n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "    \n",
      "        >>> import seaborn as sns\n",
      "        >>> sns.set(style=\"darkgrid\")\n",
      "        >>> titanic = sns.load_dataset(\"titanic\")\n",
      "        >>> ax = sns.countplot(x=\"class\", data=titanic)\n",
      "    \n",
      "    Show value counts for two categorical variables:\n",
      "    \n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "    \n",
      "        >>> ax = sns.countplot(x=\"class\", hue=\"who\", data=titanic)\n",
      "    \n",
      "    Plot the bars horizontally:\n",
      "    \n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "    \n",
      "        >>> ax = sns.countplot(y=\"class\", hue=\"who\", data=titanic)\n",
      "    \n",
      "    Use a different color palette:\n",
      "    \n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "    \n",
      "        >>> ax = sns.countplot(x=\"who\", data=titanic, palette=\"Set3\")\n",
      "    \n",
      "    Use ``plt.bar`` keyword arguments for a different look:\n",
      "    \n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "    \n",
      "        >>> ax = sns.countplot(x=\"who\", data=titanic,\n",
      "        ...                    facecolor=(0, 0, 0, 0),\n",
      "        ...                    linewidth=5,\n",
      "        ...                    edgecolor=sns.color_palette(\"dark\", 3))\n",
      "    \n",
      "    Use :func:`catplot` to combine a :func:`countplot` and a\n",
      "    :class:`FacetGrid`. This allows grouping within additional categorical\n",
      "    variables. Using :func:`catplot` is safer than using :class:`FacetGrid`\n",
      "    directly, as it ensures synchronization of variable order across facets:\n",
      "    \n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "    \n",
      "        >>> g = sns.catplot(x=\"class\", hue=\"who\", col=\"survived\",\n",
      "        ...                 data=titanic, kind=\"count\",\n",
      "        ...                 height=4, aspect=.7);\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# basic plots: How do various attributes vary with the digits\n",
    "\n",
    "\n",
    "#sns.barplot(x='label', y='pixel12', data=digit, order=order)\n",
    "#sns.countplot(digit)\n",
    "help(sns.countplot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x264062bedd8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7wAAAHjCAYAAAAABBM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAG6xJREFUeJzt3X+sZ3dd5/HXm04RQbGFjmztVKe7NoTqqkBT0CaorUJBpWiKKRFtWDY1u8WFXbMuaLLgj24kq4vKKklDK0VZai26VEPEhl+ubvgx5XepbCsiHVvpsC0guoLF9/5xT/UC0+mF3u/93vvu45Hc3O/3c8739j0nk+k855zv+VZ3BwAAAKZ5wLoHAAAAgFUQvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABG2rfuAVbhpJNO6oMHD657DAAAAFbg+uuv/1h377+3/UYG78GDB3Po0KF1jwEAAMAKVNVfbGU/lzQDAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBI+9Y9ALN95Gf+5bpH2LW+9j+/b90jAADAaM7wAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGGnfugcAAICtetGLXrTuEXY1xwc+lzO8AAAAjCR4AQAAGEnwAgAAMJL38ALci7c84dvXPcKu9u1/9JZ1jwAAcFTO8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAI+1b9wAAAAD3N998zevXPcKu9Z4LnrRtP0vwwh539kvPXvcIu9af/NifrHsEAADWyCXNAAAAjCR4AQAAGEnwAgAAMNL99j28j/2Pr1z3CLva9f/1R9Y9AgDsOTde+sZ1j7BrPeqnzln3CMD9kDO8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEj327s0A7B7/Pcf/711j7CrPecXv2/dIwDAniR4AQCAf3T1b5+17hF2tR98+tvXPQJfBJc0AwAAMNLKg7eqjquqd1XV7y/PT6uqt1XVTVX1W1X1wGX9y5bnNy/bD276GS9Y1j9YVU9a9cwAAADsfTtxhve5SW7c9PzFSV7S3acnuTPJs5f1Zye5s7u/PslLlv1SVWckuTDJNyQ5L8mvVdVxOzA3AAAAe9hKg7eqDiT5niQvX55XknOSXLPscmWSpy2Pz1+eZ9l+7rL/+Umu6u5Pd/efJ7k5iTcWAAAAcEyrPsP7S0l+Isk/LM8fnuTj3X3X8vxwklOWx6ckuSVJlu2fWPb/x/WjvOYfVdXFVXWoqg4dOXJku38dAAAA7DErC96q+t4kt3f39ZuXj7Jr38u2Y73mnxa6L+vuM7v7zP3793/R8wIAADDLKj+W6OwkT62qpyR5UJKHZuOM7wlVtW85i3sgya3L/oeTnJrkcFXtS/JVSe7YtH63za8BAACAo1rZGd7ufkF3H+jug9m46dQbu/uHkrwpyQXLbhclee3y+NrleZbtb+zuXtYvXO7ifFqS05P48CsAAACOaZVneO/Jf0pyVVX9XJJ3Jbl8Wb88yW9U1c3ZOLN7YZJ09w1VdXWSDyS5K8kl3f3ZnR8bAACAvWRHgre735zkzcvjD+Uod1nu7r9L8vR7eP2lSS5d3YQAAABMsxOfwwsAAAA7TvACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEj71j0AALAzLn3mBeseYdf6qd+8Zt0jALACzvACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGCklQVvVT2oqt5eVe+pqhuq6qeX9dOq6m1VdVNV/VZVPXBZ/7Ll+c3L9oObftYLlvUPVtWTVjUzAAAAc6zyDO+nk5zT3d+c5FuSnFdVj0/y4iQv6e7Tk9yZ5NnL/s9Ocmd3f32Slyz7parOSHJhkm9Icl6SX6uq41Y4NwAAAAOsLHh7w6eWp8cvX53knCTXLOtXJnna8vj85XmW7edWVS3rV3X3p7v7z5PcnOSsVc0NAADADCt9D29VHVdV705ye5LrkvxZko93913LLoeTnLI8PiXJLUmybP9EkodvXj/KawAAAOCoVhq83f3Z7v6WJAeycVb2UUfbbfle97DtntY/R1VdXFWHqurQkSNHvtSRAQAAGGJH7tLc3R9P8uYkj09yQlXtWzYdSHLr8vhwklOTZNn+VUnu2Lx+lNds/m9c1t1ndveZ+/fvX8UvAwAAgD1klXdp3l9VJyyPvzzJdyW5Mcmbklyw7HZRktcuj69dnmfZ/sbu7mX9wuUuzqclOT3J21c1NwAAADPsu/ddvmQnJ7lyuaPyA5Jc3d2/X1UfSHJVVf1ckncluXzZ//Ikv1FVN2fjzO6FSdLdN1TV1Uk+kOSuJJd092dXODcAAAADrCx4u/u9SR59lPUP5Sh3We7uv0vy9Hv4WZcmuXS7ZwQAAGCuHXkPLwAAAOw0wQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYaUvBW1Vv2MoaAAAA7Bb7jrWxqh6U5MFJTqqqE5PUsumhSb5mxbMBAADAl+yYwZvkR5M8Lxtxe33+KXg/meRXVzgXAAAA3CfHDN7u/uUkv1xVP9bdL92hmQAAAOA+u7czvEmS7n5pVX1bkoObX9Pdr1zRXAAAAHCfbCl4q+o3kvyLJO9O8tlluZMIXgAAAHalLQVvkjOTnNHdvcphAAAAYLts9XN435/kn61yEAAAANhOWz3De1KSD1TV25N8+u7F7n7qSqYCAACA+2irwfuiVQ4BAAAA222rd2l+y6oHAQAAgO201bs0/3U27sqcJA9McnySv+nuh65qMAAAALgvtnqG9ys3P6+qpyU5ayUTAQAAwDbY6l2aP0d3/88k52zzLAAAALBttnpJ8w9sevqAbHwur8/kBQAAYNfa6l2av2/T47uSfDjJ+ds+DQAAAGyTrb6H91mrHgQAAAC205bew1tVB6rqd6vq9qr6aFW9pqoOrHo4AAAA+FJt9aZVv57k2iRfk+SUJL+3rAEAAMCutNXg3d/dv97ddy1fr0iyf4VzAQAAwH2y1eD9WFU9s6qOW76emeT/rnIwAAAAuC+2Grz/KskPJvmrJLcluSCJG1kBAACwa231Y4l+NslF3X1nklTVw5L8QjZCGAAAAHadrZ7h/aa7YzdJuvuOJI9ezUgAAABw3201eB9QVSfe/WQ5w7vVs8MAAACw47Yarb+Y5H9X1TVJOhvv5710ZVMBAADAfbSl4O3uV1bVoSTnJKkkP9DdH1jpZAAAAHAfbPmy5CVwRS4AAAB7wlbfwwsAAAB7iuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjrSx4q+rUqnpTVd1YVTdU1XOX9YdV1XVVddPy/cRlvarqV6rq5qp6b1U9ZtPPumjZ/6aqumhVMwMAADDHKs/w3pXkx7v7UUken+SSqjojyfOTvKG7T0/yhuV5kjw5yenL18VJXpZsBHKSFyZ5XJKzkrzw7kgGAACAe7Ky4O3u27r7ncvjv05yY5JTkpyf5MpltyuTPG15fH6SV/aGtyY5oapOTvKkJNd19x3dfWeS65Kct6q5AQAAmGFH3sNbVQeTPDrJ25I8ortvSzaiOMlXL7udkuSWTS87vKzd0/rn/zcurqpDVXXoyJEj2/1LAAAAYI9ZefBW1VckeU2S53X3J4+161HW+hjrn7vQfVl3n9ndZ+7fv/9LGxYAAIAxVhq8VXV8NmL3Vd39O8vyR5dLlbN8v31ZP5zk1E0vP5Dk1mOsAwAAwD1a5V2aK8nlSW7s7v+2adO1Se6+0/JFSV67af1Hlrs1Pz7JJ5ZLnl+f5IlVdeJys6onLmsAAABwj/at8GefneSHk7yvqt69rP1kkp9PcnVVPTvJR5I8fdn2uiRPSXJzkr9N8qwk6e47qupnk7xj2e9nuvuOFc4NAADAACsL3u7+4xz9/bdJcu5R9u8kl9zDz7oiyRXbNx0AAADT7chdmgEAAGCnCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASCsL3qq6oqpur6r3b1p7WFVdV1U3Ld9PXNarqn6lqm6uqvdW1WM2veaiZf+bquqiVc0LAADALKs8w/uKJOd93trzk7yhu09P8obleZI8Ocnpy9fFSV6WbARykhcmeVySs5K88O5IBgAAgGNZWfB29x8luePzls9PcuXy+MokT9u0/sre8NYkJ1TVyUmelOS67r6ju+9Mcl2+MKIBAADgC+z0e3gf0d23Jcny/auX9VOS3LJpv8PL2j2tf4GquriqDlXVoSNHjmz74AAAAOwtu+WmVXWUtT7G+hcudl/W3Wd295n79+/f1uEAAADYe3Y6eD+6XKqc5fvty/rhJKdu2u9AkluPsQ4AAADHtNPBe22Su++0fFGS125a/5Hlbs2PT/KJ5ZLn1yd5YlWduNys6onLGgAAABzTvlX94Kp6dZLvSHJSVR3Oxt2Wfz7J1VX17CQfSfL0ZffXJXlKkpuT/G2SZyVJd99RVT+b5B3Lfj/T3Z9/IywAAAD4AisL3u5+xj1sOvco+3aSS+7h51yR5IptHA0AAID7gd1y0yoAAADYVoIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwAAgJEELwAAACMJXgAAAEYSvAAAAIwkeAEAABhJ8AIAADCS4AUAAGAkwQsAAMBIghcAAICRBC8AAAAjCV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkQQvAAAAIwleAAAARhK8AAAAjCR4AQAAGEnwAgAAMJLgBQAAYKQ9E7xVdV5VfbCqbq6q5697HgAAAHa3PRG8VXVckl9N8uQkZyR5RlWdsd6pAAAA2M32RPAmOSvJzd39oe7+TJKrkpy/5pkAAADYxaq71z3DvaqqC5Kc193/enn+w0ke193P2bTPxUkuXp4+MskHd3zQ++akJB9b9xDDOcY7w3FePcd49Rzj1XOMd4bjvHqO8eo5xqu3F4/x13X3/nvbad9OTLIN6ihrn1Pq3X1Zkst2ZpztV1WHuvvMdc8xmWO8Mxzn1XOMV88xXj3HeGc4zqvnGK+eY7x6k4/xXrmk+XCSUzc9P5Dk1jXNAgAAwB6wV4L3HUlOr6rTquqBSS5Mcu2aZwIAAGAX2xOXNHf3XVX1nCSvT3Jckiu6+4Y1j7Xd9uzl2HuIY7wzHOfVc4xXzzFePcd4ZzjOq+cYr55jvHpjj/GeuGkVAAAAfLH2yiXNAAAA8EURvAAAAIwkeNesqs6rqg9W1c1V9fx1zzNRVV1RVbdX1fvXPctUVXVqVb2pqm6sqhuq6rnrnmmaqnpQVb29qt6zHOOfXvdMU1XVcVX1rqr6/XXPMlVVfbiq3ldV766qQ+ueZ6KqOqGqrqmqP13+bP7Wdc80TVU9cvk9fPfXJ6vqeeuea5qq+vfL//feX1WvrqoHrXumaarqucvxvWHi72Hv4V2jqjouyf9J8t3Z+OildyR5Rnd/YK2DDVNVT0jyqSSv7O5vXPc8E1XVyUlO7u53VtVXJrk+ydP8Xt4+VVVJHtLdn6qq45P8cZLndvdb1zzaOFX1H5KcmeSh3f29655noqr6cJIzu/tj655lqqq6Msn/6u6XL59w8eDu/vi655pq+TvdXyZ5XHf/xbrnmaKqTsnG/+/O6O7/V1VXJ3ldd79ivZPNUVXfmOSqJGcl+UySP0jyb7r7prUOto2c4V2vs5Lc3N0f6u7PZOM32/lrnmmc7v6jJHese47Juvu27n7n8vivk9yY5JT1TjVLb/jU8vT45cu/WG6zqjqQ5HuSvHzds8CXqqoemuQJSS5Pku7+jNhduXOT/JnYXYl9Sb68qvYleXCSW9c8zzSPSvLW7v7b7r4ryVuSfP+aZ9pWgne9Tklyy6bnhyMS2OOq6mCSRyd523onmWe51PbdSW5Pcl13O8bb75eS/ESSf1j3IMN1kj+squur6uJ1DzPQP09yJMmvL5fnv7yqHrLuoYa7MMmr1z3ENN39l0l+IclHktyW5BPd/YfrnWqc9yd5QlU9vKoenOQpSU5d80zbSvCuVx1lzRkb9qyq+ookr0nyvO7+5Lrnmaa7P9vd35LkQJKzlsuQ2CZV9b1Jbu/u69c9y/3A2d39mCRPTnLJ8tYTts++JI9J8rLufnSSv0niPiErslwy/tQkv73uWaapqhOzcfXjaUm+JslDquqZ651qlu6+McmLk1yXjcuZ35PkrrUOtc0E73odzuf+C8qBuEyDPWp5X+lrkryqu39n3fNMtlya+OYk5615lGnOTvLU5f2lVyU5p6p+c70jzdTdty7fb0/yu9l4iw/b53CSw5uuArkmGwHMajw5yTu7+6PrHmSg70ry5919pLv/PsnvJPm2Nc80Tndf3t2P6e4nZONtgGPev5sI3nV7R5LTq+q05V8HL0xy7Zpngi/ackOly5Pc2N3/bd3zTFRV+6vqhOXxl2fjLwF/ut6pZunuF3T3ge4+mI0/j9/Y3c4kbLOqeshyc7ssl9k+MRuX1LFNuvuvktxSVY9cls5N4iaCq/OMuJx5VT6S5PFV9eDl7xrnZuM+IWyjqvrq5fvXJvmBDPv9vG/dA9yfdfddVfWcJK9PclySK7r7hjWPNU5VvTrJdyQ5qaoOJ3lhd1++3qnGOTvJDyd53/Ie0yT5ye5+3RpnmubkJFcudwJ9QJKru9vH5rAXPSLJ72783TX7kvyP7v6D9Y400o8ledXyD+ofSvKsNc8z0vKex+9O8qPrnmWi7n5bVV2T5J3ZuMz2XUkuW+9UI72mqh6e5O+TXNLdd657oO3kY4kAAAAYySXNAAAAjCR4AQAAGEnwAgAAMJLgBQAAYCTBCwAAwEiCFwB2qar61L1sP1hVX9Rn2FbVK6rqgvs2GQDsDYIXAACAkQQvAOxyVfUVVfWGqnpnVb2vqs7ftHlfVV1ZVe+tqmuq6sHLax5bVW+pquur6vVVdfKaxgeAtRG8ALD7/V2S7+/uxyT5ziS/WFW1bHtkksu6+5uSfDLJv62q45O8NMkF3f3YJFckuXQNcwPAWu1b9wAAwL2qJP+lqp6Q5B+SnJLkEcu2W7r7T5bHv5nk3yX5gyTfmOS6pYuPS3Lbjk4MALuA4AWA3e+HkuxP8tju/vuq+nCSBy3b+vP27WwE8g3d/a07NyIA7D4uaQaA3e+rkty+xO53Jvm6Tdu+tqruDttnJPnjJB9Msv/u9ao6vqq+YUcnBoBdQPACwO73qiRnVtWhbJzt/dNN225MclFVvTfJw5K8rLs/k+SCJC+uqvckeXeSb9vhmQFg7ar786+EAgAAgL3PGV4AAABGErwAAACMJHgBAAAYSfACAAAwkuAFAABgJMELAADASIIXAACAkf4/8JUQhocT5PIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the count of each labels in the entire dataset\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.countplot(x='label',data=digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into 'x' and 'y'\n",
    "x = digit.drop('label', axis=1)\n",
    "y = digit['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by the scale function.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Scaling the columns\n",
    "x_scaled = scale(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As train data is of 42000 rows and running gridsearch with K=5 cross validation would take hours for execution,\n",
    "thus taking 20% sample from train dataset and 80% as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into 'train' and 'test'\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, train_size=0.2, test_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of train data\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33600, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of test data\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model\n",
    "model_linear = SVC(kernel='linear')\n",
    "model_linear.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model_linear.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9106845238095238 \n",
      "\n",
      "[[3194    0   23    6   10   21   25    2    9    0]\n",
      " [   0 3669   21   13    3    5    1    6   26    4]\n",
      " [  44   24 3005   85   44   10   32   26   50   12]\n",
      " [  20   18   95 3131    4  129    5   19   61   29]\n",
      " [  14   10   36    7 2996   12   19   14    7  129]\n",
      " [  29   19   27  145   32 2701   51    6   62   17]\n",
      " [  55   10   53    4   28   54 3071    0   10    0]\n",
      " [  10   28   40   31   66    3    1 3169    5  144]\n",
      " [  25   68   57  151   22  111   26   23 2733   25]\n",
      " [  23   10   23   33  160    7    0  153   24 2930]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95      3290\n",
      "           1       0.95      0.98      0.97      3748\n",
      "           2       0.89      0.90      0.90      3332\n",
      "           3       0.87      0.89      0.88      3511\n",
      "           4       0.89      0.92      0.91      3244\n",
      "           5       0.88      0.87      0.88      3089\n",
      "           6       0.95      0.93      0.94      3285\n",
      "           7       0.93      0.91      0.92      3497\n",
      "           8       0.91      0.84      0.88      3241\n",
      "           9       0.89      0.87      0.88      3363\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     33600\n",
      "   macro avg       0.91      0.91      0.91     33600\n",
      "weighted avg       0.91      0.91      0.91     33600\n",
      "\n",
      "Precision Score ::  0.9106845238095238 \n",
      "\n",
      "Recall Score ::  0.9106845238095238 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1052: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Users\\engel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1052: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix and accuracy\n",
    "\n",
    "#1. Accuracy\n",
    "print('accuracy:', metrics.accuracy_score(y_true=y_test, y_pred=y_pred), '\\n')\n",
    "\n",
    "#2. Confusion matrix\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "#3. Class accuracy\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "#Checking the precison and recall values\n",
    "print(\"Precision Score :: \",metrics.precision_score(y_test,y_pred,pos_label='positive',average='micro'),\"\\n\")\n",
    "\n",
    "print(\"Recall Score :: \",metrics.recall_score(y_test,y_pred,pos_label='positive',average='micro'),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that linear model gives an accuracy of 91%. Lets check with non-linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function accuracy_score in module sklearn.metrics.classification:\n",
      "\n",
      "accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "    Accuracy classification score.\n",
      "    \n",
      "    In multilabel classification, this function computes subset accuracy:\n",
      "    the set of labels predicted for a sample must *exactly* match the\n",
      "    corresponding set of labels in y_true.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) labels.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Predicted labels, as returned by a classifier.\n",
      "    \n",
      "    normalize : bool, optional (default=True)\n",
      "        If ``False``, return the number of correctly classified samples.\n",
      "        Otherwise, return the fraction of correctly classified samples.\n",
      "    \n",
      "    sample_weight : array-like of shape = [n_samples], optional\n",
      "        Sample weights.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    score : float\n",
      "        If ``normalize == True``, return the fraction of correctly\n",
      "        classified samples (float), else returns the number of correctly\n",
      "        classified samples (int).\n",
      "    \n",
      "        The best performance is 1 with ``normalize == True`` and the number\n",
      "        of samples with ``normalize == False``.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    jaccard_similarity_score, hamming_loss, zero_one_loss\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In binary and multiclass classification, this function is equal\n",
      "    to the ``jaccard_similarity_score`` function.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.metrics import accuracy_score\n",
      "    >>> y_pred = [0, 2, 1, 3]\n",
      "    >>> y_true = [0, 1, 2, 3]\n",
      "    >>> accuracy_score(y_true, y_pred)\n",
      "    0.5\n",
      "    >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "    2\n",
      "    \n",
      "    In the multilabel case with binary label indicators:\n",
      "    \n",
      "    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "    0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(metrics.accuracy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engel\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Non-linear model\n",
    "# using rbf kernel, C=1, default value of gamma\n",
    "\n",
    "# model\n",
    "non_linear_model = SVC(kernel='rbf')\n",
    "\n",
    "# fitting model\n",
    "non_linear_model.fit(x_train,y_train )\n",
    "\n",
    "#Predict\n",
    "y_pred = non_linear_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9340773809523809 \n",
      "\n",
      "[[3199    0   32    2    3   13   28    1   12    0]\n",
      " [   0 3654   39   12    5    9    6    4   16    3]\n",
      " [  15    8 3190   21   22    2   16   22   29    7]\n",
      " [   5   14  123 3187    4   65    8   32   56   17]\n",
      " [   3    8   63    0 3029   10   18   13    7   93]\n",
      " [  12    7   47   82   17 2841   49    6   17   11]\n",
      " [  18    6   67    0   13   32 3140    0    9    0]\n",
      " [   1   26  117   11   21    3    1 3192    2  123]\n",
      " [  20   36   71   56   16   73   14   12 2918   25]\n",
      " [  14   12   64   38   56   10    1  109   24 3035]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      3290\n",
      "           1       0.97      0.97      0.97      3748\n",
      "           2       0.84      0.96      0.89      3332\n",
      "           3       0.93      0.91      0.92      3511\n",
      "           4       0.95      0.93      0.94      3244\n",
      "           5       0.93      0.92      0.92      3089\n",
      "           6       0.96      0.96      0.96      3285\n",
      "           7       0.94      0.91      0.93      3497\n",
      "           8       0.94      0.90      0.92      3241\n",
      "           9       0.92      0.90      0.91      3363\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     33600\n",
      "   macro avg       0.94      0.93      0.93     33600\n",
      "weighted avg       0.94      0.93      0.93     33600\n",
      "\n",
      "Precision Score ::  0.9340773809523809 \n",
      "\n",
      "Recall Score ::  0.9340773809523809 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1052: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Users\\engel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1052: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix and accuracy\n",
    "\n",
    "#1. Accuracy\n",
    "print('accuracy:', metrics.accuracy_score(y_true=y_test, y_pred=y_pred), '\\n')\n",
    "\n",
    "#2. Confusion matrix\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "#3. Class accuracy\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "#4. Checking the precison and recall values\n",
    "print(\"Precision Score :: \",metrics.precision_score(y_test,y_pred,pos_label='positive',average='micro'),\"\\n\")\n",
    "\n",
    "print(\"Recall Score :: \",metrics.recall_score(y_test,y_pred,pos_label='positive',average='micro'),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that the non-linear model accuracy (93.4%) than the liner_model (91%).Hence chosing hyperparameters corresponding to non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search: hyperparameter tuning\n",
    "Tuning the model to find optimal values of 'C' and 'gamma' corresponding to an RBF kernel. Using 5-Fold cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 36.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'gamma': [0.01, 0.001, 0.0001], 'C': [1, 10, 100, 1000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating KFold object with 5 splits\n",
    "\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 101)\n",
    "\n",
    "# specify range of hyperparameters\n",
    "# Set the parameters by cross-validation\n",
    "hyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "\n",
    "# specify model\n",
    "model = SVC(kernel=\"rbf\")\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = model, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'accuracy', \n",
    "                        cv = folds, \n",
    "                        n_jobs=-1,\n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(x_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171.349475</td>\n",
       "      <td>1.177637</td>\n",
       "      <td>21.557189</td>\n",
       "      <td>0.104083</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 1, 'gamma': 0.01}</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.754762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750952</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.929468</td>\n",
       "      <td>0.541165</td>\n",
       "      <td>12.421936</td>\n",
       "      <td>0.133198</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 1, 'gamma': 0.001}</td>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.932738</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935238</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>4</td>\n",
       "      <td>0.975298</td>\n",
       "      <td>0.974256</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.974554</td>\n",
       "      <td>0.975446</td>\n",
       "      <td>0.974911</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.164021</td>\n",
       "      <td>0.746276</td>\n",
       "      <td>17.193949</td>\n",
       "      <td>0.503456</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'C': 1, 'gamma': 0.0001}</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.894643</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>0.008018</td>\n",
       "      <td>8</td>\n",
       "      <td>0.914732</td>\n",
       "      <td>0.917262</td>\n",
       "      <td>0.913244</td>\n",
       "      <td>0.914881</td>\n",
       "      <td>0.916220</td>\n",
       "      <td>0.915268</td>\n",
       "      <td>0.001372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176.948021</td>\n",
       "      <td>1.298124</td>\n",
       "      <td>22.040247</td>\n",
       "      <td>0.317788</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 10, 'gamma': 0.01}</td>\n",
       "      <td>0.752976</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768095</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.738320</td>\n",
       "      <td>0.477880</td>\n",
       "      <td>11.180703</td>\n",
       "      <td>0.199110</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 10, 'gamma': 0.001}</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.951786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943690</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999405</td>\n",
       "      <td>0.999554</td>\n",
       "      <td>0.999554</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.041066</td>\n",
       "      <td>0.489403</td>\n",
       "      <td>10.578899</td>\n",
       "      <td>0.059815</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'C': 10, 'gamma': 0.0001}</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.926190</td>\n",
       "      <td>0.936310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>6</td>\n",
       "      <td>0.963095</td>\n",
       "      <td>0.964137</td>\n",
       "      <td>0.962202</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.963839</td>\n",
       "      <td>0.963512</td>\n",
       "      <td>0.000773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>179.928594</td>\n",
       "      <td>8.143637</td>\n",
       "      <td>21.917523</td>\n",
       "      <td>0.291809</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 100, 'gamma': 0.01}</td>\n",
       "      <td>0.752976</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768095</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40.924681</td>\n",
       "      <td>3.662642</td>\n",
       "      <td>12.095749</td>\n",
       "      <td>0.450314</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 100, 'gamma': 0.001}</td>\n",
       "      <td>0.941071</td>\n",
       "      <td>0.945238</td>\n",
       "      <td>0.952976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943571</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.230249</td>\n",
       "      <td>0.134727</td>\n",
       "      <td>8.932539</td>\n",
       "      <td>0.058176</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'C': 100, 'gamma': 0.0001}</td>\n",
       "      <td>0.932143</td>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.936310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929048</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>5</td>\n",
       "      <td>0.995387</td>\n",
       "      <td>0.996429</td>\n",
       "      <td>0.996131</td>\n",
       "      <td>0.996280</td>\n",
       "      <td>0.994048</td>\n",
       "      <td>0.995655</td>\n",
       "      <td>0.000880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>136.536882</td>\n",
       "      <td>14.465045</td>\n",
       "      <td>13.848166</td>\n",
       "      <td>0.368960</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.01}</td>\n",
       "      <td>0.752976</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768095</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.315848</td>\n",
       "      <td>0.302884</td>\n",
       "      <td>7.035526</td>\n",
       "      <td>0.041248</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.001}</td>\n",
       "      <td>0.941071</td>\n",
       "      <td>0.945238</td>\n",
       "      <td>0.952976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943571</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.453421</td>\n",
       "      <td>0.319375</td>\n",
       "      <td>5.212445</td>\n",
       "      <td>0.137404</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.0001}</td>\n",
       "      <td>0.929167</td>\n",
       "      <td>0.929167</td>\n",
       "      <td>0.931548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925357</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0      171.349475      1.177637        21.557189        0.104083       1   \n",
       "1       41.929468      0.541165        12.421936        0.133198       1   \n",
       "2       62.164021      0.746276        17.193949        0.503456       1   \n",
       "3      176.948021      1.298124        22.040247        0.317788      10   \n",
       "4       35.738320      0.477880        11.180703        0.199110      10   \n",
       "5       28.041066      0.489403        10.578899        0.059815      10   \n",
       "6      179.928594      8.143637        21.917523        0.291809     100   \n",
       "7       40.924681      3.662642        12.095749        0.450314     100   \n",
       "8       22.230249      0.134727         8.932539        0.058176     100   \n",
       "9      136.536882     14.465045        13.848166        0.368960    1000   \n",
       "10      23.315848      0.302884         7.035526        0.041248    1000   \n",
       "11      13.453421      0.319375         5.212445        0.137404    1000   \n",
       "\n",
       "   param_gamma                        params  split0_test_score  \\\n",
       "0         0.01       {'C': 1, 'gamma': 0.01}           0.738095   \n",
       "1        0.001      {'C': 1, 'gamma': 0.001}           0.934524   \n",
       "2       0.0001     {'C': 1, 'gamma': 0.0001}           0.904167   \n",
       "3         0.01      {'C': 10, 'gamma': 0.01}           0.752976   \n",
       "4        0.001     {'C': 10, 'gamma': 0.001}           0.941667   \n",
       "5       0.0001    {'C': 10, 'gamma': 0.0001}           0.928571   \n",
       "6         0.01     {'C': 100, 'gamma': 0.01}           0.752976   \n",
       "7        0.001    {'C': 100, 'gamma': 0.001}           0.941071   \n",
       "8       0.0001   {'C': 100, 'gamma': 0.0001}           0.932143   \n",
       "9         0.01    {'C': 1000, 'gamma': 0.01}           0.752976   \n",
       "10       0.001   {'C': 1000, 'gamma': 0.001}           0.941071   \n",
       "11      0.0001  {'C': 1000, 'gamma': 0.0001}           0.929167   \n",
       "\n",
       "    split1_test_score  split2_test_score  ...  mean_test_score  \\\n",
       "0            0.773810           0.754762  ...         0.750952   \n",
       "1            0.932738           0.942857  ...         0.935238   \n",
       "2            0.894643           0.912500  ...         0.899167   \n",
       "3            0.791667           0.773810  ...         0.768095   \n",
       "4            0.946429           0.951786  ...         0.943690   \n",
       "5            0.926190           0.936310  ...         0.928571   \n",
       "6            0.791667           0.773810  ...         0.768095   \n",
       "7            0.945238           0.952976  ...         0.943571   \n",
       "8            0.934524           0.936310  ...         0.929048   \n",
       "9            0.791667           0.773810  ...         0.768095   \n",
       "10           0.945238           0.952976  ...         0.943571   \n",
       "11           0.929167           0.931548  ...         0.925357   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         0.013292               12            1.000000            0.999851   \n",
       "1         0.003973                4            0.975298            0.974256   \n",
       "2         0.008018                8            0.914732            0.917262   \n",
       "3         0.014338                9            1.000000            1.000000   \n",
       "4         0.006112                1            0.999405            0.999554   \n",
       "5         0.005120                6            0.963095            0.964137   \n",
       "6         0.014338                9            1.000000            1.000000   \n",
       "7         0.006633                2            1.000000            1.000000   \n",
       "8         0.007891                5            0.995387            0.996429   \n",
       "9         0.014338                9            1.000000            1.000000   \n",
       "10        0.006633                2            1.000000            1.000000   \n",
       "11        0.006007                7            1.000000            1.000000   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0             1.000000            0.999851            0.999851   \n",
       "1             0.975000            0.974554            0.975446   \n",
       "2             0.913244            0.914881            0.916220   \n",
       "3             1.000000            1.000000            1.000000   \n",
       "4             0.999554            0.999702            0.999702   \n",
       "5             0.962202            0.964286            0.963839   \n",
       "6             1.000000            1.000000            1.000000   \n",
       "7             1.000000            1.000000            1.000000   \n",
       "8             0.996131            0.996280            0.994048   \n",
       "9             1.000000            1.000000            1.000000   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "0           0.999911         0.000073  \n",
       "1           0.974911         0.000447  \n",
       "2           0.915268         0.001372  \n",
       "3           1.000000         0.000000  \n",
       "4           0.999583         0.000111  \n",
       "5           0.963512         0.000773  \n",
       "6           1.000000         0.000000  \n",
       "7           1.000000         0.000000  \n",
       "8           0.995655         0.000880  \n",
       "9           1.000000         0.000000  \n",
       "10          1.000000         0.000000  \n",
       "11          1.000000         0.000000  \n",
       "\n",
       "[12 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converting C to numeric type for plotting on x-axis\n",
    "cv_results['param_C'] = cv_results['param_C'].astype('int')\n",
    "\n",
    "## plotting\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# subplot 1/3\n",
    "plt.subplot(131)\n",
    "gamma_01 = cv_results[cv_results['param_gamma']==0.01]\n",
    "\n",
    "plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\n",
    "plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.01\")\n",
    "plt.ylim([0.60, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "\n",
    "# subplot 2/3\n",
    "plt.subplot(132)\n",
    "gamma_001 = cv_results[cv_results['param_gamma']==0.001]\n",
    "\n",
    "plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\n",
    "plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.001\")\n",
    "plt.ylim([0.60, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "\n",
    "\n",
    "# subplot 3/3\n",
    "plt.subplot(133)\n",
    "gamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n",
    "\n",
    "plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\n",
    "plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Gamma=0.0001\")\n",
    "plt.ylim([0.60, 1])\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plots above:\n",
    "Non-linear models (high gamma) perform much better than the linear ones At any value of gamma, a high value of C leads to better performance None of the models tend to overfit (even the complex ones), since the training and test accuracies closely follow each other This suggests that the problem and the data is inherently non-linear in nature, and a complex model will outperform simple, linear models in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best test score is 0.9436904761904762 corresponding to hyperparameters {'C': 10, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "best_score = model_cv.best_score_\n",
    "best_hyperparams = model_cv.best_params_\n",
    "\n",
    "print(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9433928571428571 \n",
      "\n",
      "[[3216    0   24    1    3   13   20    1   12    0]\n",
      " [   0 3666   33   10    5    3    5    6   16    4]\n",
      " [  14   13 3188   33   16    2   18   17   25    6]\n",
      " [   5    4   90 3249    3   52    9   22   58   19]\n",
      " [   6    8   49    0 3059    7   12   14    9   80]\n",
      " [   9    6   38   70   11 2869   47    6   19   14]\n",
      " [  26    3   56    1   14   29 3146    0   10    0]\n",
      " [   4   24   88   17   29    2    0 3268    4   61]\n",
      " [  16   19   59   61   17   65   13   10 2957   24]\n",
      " [  17    8   53   23   63    9    1   95   14 3080]] \n",
      "\n",
      "Precision Score ::  0.9433928571428571\n",
      "Recall Score ::  0.9433928571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      3290\n",
      "           1       0.98      0.98      0.98      3748\n",
      "           2       0.87      0.96      0.91      3332\n",
      "           3       0.94      0.93      0.93      3511\n",
      "           4       0.95      0.94      0.95      3244\n",
      "           5       0.94      0.93      0.93      3089\n",
      "           6       0.96      0.96      0.96      3285\n",
      "           7       0.95      0.93      0.94      3497\n",
      "           8       0.95      0.91      0.93      3241\n",
      "           9       0.94      0.92      0.93      3363\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     33600\n",
      "   macro avg       0.94      0.94      0.94     33600\n",
      "weighted avg       0.94      0.94      0.94     33600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1052: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Users\\engel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1052: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# model with optimal hyperparameters\n",
    "\n",
    "# model\n",
    "model = SVC(C=10, gamma=0.001, kernel=\"rbf\")\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# metrics\n",
    "print(\"accuracy\", metrics.accuracy_score(y_test, y_pred), \"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "\n",
    "print(\"Precision Score :: \",metrics.precision_score(y_test,y_pred,pos_label='positive',average='micro'))\n",
    "\n",
    "print(\"Recall Score :: \",metrics.recall_score(y_test,y_pred,pos_label='positive',average='micro'))\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "The accuracy achieved using a non-linear kernel (0.943) is higher than that of a linear one (0.91). Hence, the problem is highly non-linear in nature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
